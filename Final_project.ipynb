{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM2sXu0D2uUSO/ehPUDvbmU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukeD77/Machine_Learning2/blob/master/Final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXjqa-XU4ADo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "77dfe426-c026-4afc-9482-d33a5cc2e247"
      },
      "source": [
        "''' For the structure:\n",
        "DONE 1. get the data pipeline to flow into a file?\n",
        "\ta. Can we specify tweets with the word mask before we download?\n",
        "DONE 2. Import that file into pycharm\n",
        "DONE 3. Extract tweets into main dataframe\n",
        "\ta. Potentially have to filter out for tweets with just the word mask\n",
        "DONE 4. Extract the url for the image of the profile picture and put into main data frame\n",
        "5. Run connotative analysis and put the connotation in the dataframe\n",
        "\ta. check to see the unique values and throw out odd ones\n",
        "6. Use a wget command or something like that to load all the images from the urls into a dataframe\n",
        "7. Run that image list into the gender model and put the age and gender back into the main data frame\n",
        "\ta. check to see if images were succesfully identified and throw out ones that were nt\n",
        "8. Split the dataframe or do kfold validation on random forest regression'''"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' For the structure:\\nDONE 1. get the data pipeline to flow into a file?\\n\\ta. Can we specify tweets with the word mask before we download?\\nDONE 2. Import that file into pycharm\\nDONE 3. Extract tweets into main dataframe\\n\\ta. Potentially have to filter out for tweets with just the word mask\\nDONE 4. Extract the url for the image of the profile picture and put into main data frame\\n5. Run connotative analysis and put the connotation in the dataframe\\n\\ta. check to see the unique values and throw out odd ones\\n6. Use a wget command or something like that to load all the images from the urls into a dataframe\\n7. Run that image list into the gender model and put the age and gender back into the main data frame\\n\\ta. check to see if images were succesfully identified and throw out ones that were nt\\n8. Split the dataframe or do kfold validation on random forest regression'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChWzcyi14Dh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Problems ##\n",
        "# the tweets i get are truncated and cut off so maybe find function that uses the url to go get the whole tweet\n",
        "# Need to get profile_image_url to use facial recognition"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lTc80xU4IIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3pwIX1u4LD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "### Retrieve data from Twitter\n",
        "# Get authorizations\n",
        "API_key = \"m6ts9AIJlo6m3UBjA2ZKGhtIB\"\n",
        "API_secret_key = \"DJgbXa1GSDS2wSPMQWlYmAylMRJUzjE4WGLgshQtOvAGng3NdF\"\n",
        "access_token = \"1058432396482220032-ltlhoR2uAgogVZ3HrrfE5waU1SQUSD\"\n",
        "secret_access_token = \"njNuN9e6JIDFrjuYbHUDeR400z3tCX12I2Vg0AusFWkD5\"\n",
        "\n",
        "# Set up API\n",
        "auth = tweepy.OAuthHandler(API_key, API_secret_key)\n",
        "auth.set_access_token(access_token, secret_access_token)\n",
        "api = tweepy.API(auth)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlQJ1vEm4Rzq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "00de71d9-f8d6-4569-cdd1-84bd12d62818"
      },
      "source": [
        "# Search for tweets with the phrase \"wear a mask\" and put the tweet and profile image URL into a dataframe\n",
        "tweet_list = []\n",
        "image_list = []\n",
        "for status in tweepy.Cursor(api.search,q='wear a mask',count=100, since=2020-3-1).items():\n",
        "    tweet_list.append(status.text)\n",
        "    image_list.append(status.user.profile_image_url)\n",
        "\n",
        "# I could also do this again with the phrase \"wearing a mask\", \"wore a mask\" and such to expand dataset\n",
        "\n",
        "len(tweet_list)\n",
        "len(image_list)\n",
        "main_df = pd.DataFrame()\n",
        "main_df['tweets'] = tweet_list\n",
        "main_df['images'] = image_list\n",
        "main_df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TweepError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTweepError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-478956fcffae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtweet_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimage_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wear a mask'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msince\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtweet_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimage_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile_image_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;31m# Reached end of current page, get the next page...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__self__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m# Set pagination mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# Parse the response payload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTweepError\u001b[0m: Twitter error response: status code = 429"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu5SLhMc4WZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "### Using sentiment analysis model from tensorflow Text Classification with RNN\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n",
        "  plt.show()\n",
        "\n",
        "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "encoder = info.features['text'].encoder\n",
        "\n",
        "for index in encoded_string:\n",
        "  print('{} ----> {}'.format(index, encoder.decode([index])))\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
        "\n",
        "test_dataset = test_dataset.padded_batch(BATCH_SIZE)\n",
        "\n",
        "\n",
        "def pad_to_size(vec, size):\n",
        "  zeros = [0] * (size - len(vec))\n",
        "  vec.extend(zeros)\n",
        "  return vec\n",
        "\n",
        "def sample_predict(sample_pred_text, pad):\n",
        "  encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
        "\n",
        "  if pad:\n",
        "    encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
        "  encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
        "  predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
        "\n",
        "  return (predictions)\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HXLCIH64Xab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run the model\n",
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_ezC7Zz4cX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))\n",
        "\n",
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_pred_text = ('The movie was not good. The animation and the graphics '\n",
        "                    'were terrible. I would not recommend this movie.')\n",
        "predictions = sample_predict(sample_pred_text, pad=False)\n",
        "print(predictions)\n",
        "\n",
        "# predict on a sample text with padding\n",
        "\n",
        "sample_pred_text = ('The movie was not good. The animation and the graphics '\n",
        "                    'were terrible. I would not recommend this movie.')\n",
        "predictions = sample_predict(sample_pred_text, pad=True)\n",
        "print(predictions)\n",
        "\n",
        "plot_graphs(history, 'accuracy')\n",
        "\n",
        "plot_graphs(history, 'loss')\n",
        "\n",
        "pred_text = main_df['tweets'][0]\n",
        "pred_text\n",
        "\n",
        "predictions = sample_predict(pred_text, pad=True)\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucLZxRBw6VPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_list = []\n",
        "for tweet in main_df['tweets']:\n",
        "  pred_text = tweet\n",
        "  prediction = sample_predict(pred_text, pad = True)\n",
        "  sentiment_list.append(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf7n8dhF68Oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for value in sentiment_list:\n",
        "  if value < 0.5:\n",
        "    value = 'Negative'\n",
        "  else:\n",
        "    value = 'Positive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRQ7gw2B4gSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "########### Junk Yard ##############\n",
        "\n",
        "#\n",
        "# api = twitter.Api(consumer_key=API_key,\n",
        "#                   consumer_secret=API_secret_key,\n",
        "#                   access_token_key=access_token,\n",
        "#                   access_token_secret=secret_access_token,\n",
        "#                   tweet_mode='extended')\n",
        "#\n",
        "# data = api.GetSearch(term='mask',count=100, since=2020-3-1)\n",
        "# data2 = api.GetSearch(term='masks', since=2020-3-1)\n",
        "# data3 = api.GetStatus()\n",
        "#\n",
        "# #data = data.append(data2)\n",
        "# main_df = pd.DataFrame()\n",
        "#\n",
        "# # extract tweet\n",
        "# print(data)\n",
        "# len(data)\n",
        "# print(data2)\n",
        "# test = data[0]\n",
        "# test\n",
        "# test[0]\n",
        "#\n",
        "# print(test.text)\n",
        "#\n",
        "# for status in api.Cursor(api.GetSearc)\n",
        "#     n = n+1\n",
        "#     print(n)\n",
        "#\n",
        "# # This does not work as a for loop because indexing data? changed to while loop and integer values and still didn't take\n",
        "# l_tweets = []\n",
        "# for x in data:\n",
        "#     a = str(data[x])\n",
        "#     b = a.split(\"text\")\n",
        "#     c = b[1].split('urls')\n",
        "#     d = c[0].split()\n",
        "#     e = d[1:-1]\n",
        "#     f = ' '.join(e)\n",
        "#     g = f[:-2]\n",
        "#     h = g[1:]\n",
        "#     tweet = h\n",
        "#     l_tweets.append(tweet)\n",
        "#\n",
        "#\n",
        "# # By hand\n",
        "# a = str(data[2])\n",
        "# b = a.split(\"text\")\n",
        "# c = b[1].split('urls')\n",
        "# d = c[0].split()\n",
        "# e = d[1:-1]\n",
        "# f = ' '.join(e)\n",
        "# g = f[:-2]\n",
        "# h = g[1:]\n",
        "# tweet = h\n",
        "# l_tweets.append(tweet)\n",
        "#\n",
        "# main_df['tweets'] = l_tweets\n",
        "# main_df\n",
        "# pd.options.display.max_colwidth=300\n",
        "# print(main_df.iloc[0],)\n",
        "#\n",
        "#\n",
        "# a = str(data[3])\n",
        "# b = a.split(\"text\")\n",
        "# c = b[1].split('urls')\n",
        "# d = c[0].split()\n",
        "# e = d[1:-1]\n",
        "# f = ' '.join(e)\n",
        "# g = f[:-2]\n",
        "# h = g[1:]\n",
        "# h\n",
        "# print(a)\n",
        "# data[3]\n",
        "#\n",
        "# a = str(data2[3])\n",
        "# b = a.split(\"text\")\n",
        "# c = b[1].split('urls')\n",
        "# d = c[0].split()\n",
        "# e = d[1:-1]\n",
        "# f = ' '.join(e)\n",
        "# g = f[:-2]\n",
        "# h = g[1:]\n",
        "# h\n",
        "# print(a)\n",
        "# data[3]\n",
        "#\n",
        "# data_dct = dict()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}